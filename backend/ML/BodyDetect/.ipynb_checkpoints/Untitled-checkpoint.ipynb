{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df7e9be-5020-4019-9087-c5b834b1f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyAudio in c:\\users\\lenevo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be69874f-5dba-40aa-a3a9-59266e715204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talk\n",
      "Time over, thanks\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize recognizer class (for recognizing the speech)\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Reading Microphone as source\n",
    "# listening the speech and store in audio_text variable\n",
    "\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Talk\")\n",
    "    r.pause_threshold = 5\n",
    "    audio_text = r.listen(source, timeout=10)\n",
    "    print(\"Time over, thanks\")\n",
    "# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling\n",
    "    \n",
    "    try:\n",
    "        # using google speech recognition\n",
    "        f= open(\"Answer.txt\",\"w+\")\n",
    "        f.write(r.recognize_google(audio_text))\n",
    "        print(\"done\")\n",
    "        f.close()\n",
    "    except:\n",
    "         print(\"Sorry, I did not get that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7120f507-ac81-4944-ac38-b97f9fd12a30",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m         unique\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[0;32m     15\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltered.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(word)\n\u001b[0;32m     19\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "text_file = open('Answer.txt', 'r')\n",
    "text = text_file.read()\n",
    "\n",
    "# Cleaning\n",
    "text = text.lower()\n",
    "words = text.split()\n",
    "words = [word.strip('.,!;()[]') for word in words]\n",
    "words = [word.replace(\"'s\", '') for word in words]\n",
    "\n",
    "# Finding unique\n",
    "unique = []\n",
    "for word in words:\n",
    "    if word not in unique:\n",
    "        unique.append(word)\n",
    "        f = open(\"Filtered.txt\",\"a\")\n",
    "        f.append(word)\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b614c44-aa79-4d22-bf98-b7c15f057bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenevo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\lenevo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenevo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenevo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenevo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenevo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0921fd8-b4f9-421e-92ae-da13267e5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd18d1bd-c494-4793-92df-7acd89c3cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a2d5c-59bd-4e3b-ac13-d2c1406a0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "file1 = open(\"Answer.txt\") \n",
    " \n",
    "# Use this to read file content as a stream: \n",
    "line = file1.read()\n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('filteredtext.txt','a+') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49b693e-570c-4d2c-8125-81081e59a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "num_words = 0\n",
    " \n",
    "with open(\"Keywords.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        words = line.split()\n",
    "        num_words += len(words)\n",
    "print(\"Number of words:\")\n",
    "print(num_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b504b85c-019a-4a6a-a945-df67ca78c357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Answer.txt :\n",
      "11 lines, \n",
      "3 words, \n",
      "3 distinct words\n",
      "File Keywords.txt :\n",
      "11 lines, \n",
      "3 words, \n",
      "3 distinct words\n",
      "The distance between the documents is:  0.000000 (radians)\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import string \n",
    "import sys \n",
    "\n",
    "# reading the text file \n",
    "# This functio will return a \n",
    "# list of the lines of text \n",
    "# in the file. \n",
    "def read_file(filename): \n",
    "\t\n",
    "\ttry: \n",
    "\t\twith open(filename, 'r') as f: \n",
    "\t\t\tdata = f.read() \n",
    "\t\treturn data \n",
    "\t\n",
    "\texcept IOError: \n",
    "\t\tprint(\"Error opening or reading input file: \", filename) \n",
    "\t\tsys.exit() \n",
    "\n",
    "# splitting the text lines into words \n",
    "# translation table is a global variable \n",
    "# mapping upper case to lower case and \n",
    "# punctuation to spaces \n",
    "translation_table = str.maketrans(string.punctuation+string.ascii_uppercase, \n",
    "\t\t\t\t\t\t\t\t\t\" \"*len(string.punctuation)+string.ascii_lowercase) \n",
    "\t\n",
    "# returns a list of the words \n",
    "# in the file \n",
    "def get_words_from_line_list(text): \n",
    "\t\n",
    "\ttext = text.translate(translation_table) \n",
    "\tword_list = text.split() \n",
    "\t\n",
    "\treturn word_list \n",
    "\n",
    "\n",
    "# counts frequency of each word \n",
    "# returns a dictionary which maps \n",
    "# the words to their frequency. \n",
    "def count_frequency(word_list): \n",
    "\t\n",
    "\tD = {} \n",
    "\t\n",
    "\tfor new_word in word_list: \n",
    "\t\t\n",
    "\t\tif new_word in D: \n",
    "\t\t\tD[new_word] = D[new_word] + 1\n",
    "\t\t\t\n",
    "\t\telse: \n",
    "\t\t\tD[new_word] = 1\n",
    "\t\t\t\n",
    "\treturn D \n",
    "\n",
    "# returns dictionary of (word, frequency) \n",
    "# pairs from the previous dictionary. \n",
    "def word_frequencies_for_file(filename): \n",
    "\t\n",
    "\tline_list = read_file(filename) \n",
    "\tword_list = get_words_from_line_list(line_list) \n",
    "\tfreq_mapping = count_frequency(word_list) \n",
    "\n",
    "\tprint(\"File\", filename, \":\", ) \n",
    "\tprint(len(line_list), \"lines, \", ) \n",
    "\tprint(len(word_list), \"words, \", ) \n",
    "\tprint(len(freq_mapping), \"distinct words\") \n",
    "\n",
    "\treturn freq_mapping \n",
    "\n",
    "\n",
    "# returns the dot product of two documents \n",
    "def dotProduct(D1, D2): \n",
    "\tSum = 0.0\n",
    "\t\n",
    "\tfor key in D1: \n",
    "\t\t\n",
    "\t\tif key in D2: \n",
    "\t\t\tSum += (D1[key] * D2[key]) \n",
    "\t\t\t\n",
    "\treturn Sum\n",
    "\n",
    "# returns the angle in radians \n",
    "# between document vectors \n",
    "def vector_angle(D1, D2): \n",
    "\tnumerator = dotProduct(D1, D2) \n",
    "\tdenominator = math.sqrt(dotProduct(D1, D1)*dotProduct(D2, D2)) \n",
    "\t\n",
    "\treturn math.acos(numerator / denominator) \n",
    "\n",
    "\n",
    "def documentSimilarity(filename_1, filename_2): \n",
    "\t\n",
    "# filename_1 = sys.argv[1] \n",
    "# filename_2 = sys.argv[2] \n",
    "\tsorted_word_list_1 = word_frequencies_for_file(filename_1) \n",
    "\tsorted_word_list_2 = word_frequencies_for_file(filename_2) \n",
    "\tdistance = vector_angle(sorted_word_list_1, sorted_word_list_2) \n",
    "\t\n",
    "\tprint(\"The distance between the documents is: % 0.6f (radians)\"% distance) \n",
    "\t\n",
    "# Driver code \n",
    "documentSimilarity('Answer.txt', 'Keywords.txt') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e25362-bafd-4c55-bbfb-504cec7955e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
