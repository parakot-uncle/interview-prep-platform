{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df7e9be-5020-4019-9087-c5b834b1f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyAudio in c:\\users\\asus\\desktop\\ml\\da_lab_ese\\env\\lib\\site-packages (0.2.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be69874f-5dba-40aa-a3a9-59266e715204",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'speech_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import library\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspeech_recognition\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msr\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize recognizer class (for recognizing the speech)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m r \u001b[38;5;241m=\u001b[39m sr\u001b[38;5;241m.\u001b[39mRecognizer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'speech_recognition'"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize recognizer class (for recognizing the speech)\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Reading Microphone as source\n",
    "# listening the speech and store in audio_text variable\n",
    "\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Talk\")\n",
    "    r.pause_threshold = 5\n",
    "    audio_text = r.listen(source, timeout=10)\n",
    "    print(\"Time over, thanks\")\n",
    "# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling\n",
    "    \n",
    "    try:\n",
    "        # using google speech recognition\n",
    "        f= open(\"Answer.txt\",\"w+\")\n",
    "        f.write(r.recognize_google(audio_text))\n",
    "        print(\"done\")\n",
    "        f.close()\n",
    "    except:\n",
    "         print(\"Sorry, I did not get that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120f507-ac81-4944-ac38-b97f9fd12a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('Answer.txt', 'r')\n",
    "text = text_file.read()\n",
    "\n",
    "# Cleaning\n",
    "text = text.lower()\n",
    "words = text.split()\n",
    "words = [word.strip('.,!;()[]') for word in words]\n",
    "words = [word.replace(\"'s\", '') for word in words]\n",
    "\n",
    "# Finding unique\n",
    "unique = []\n",
    "for word in words:\n",
    "    if word not in unique:\n",
    "        unique.append(word)\n",
    "        f = open(\"Filtered.txt\",\"a+\")\n",
    "        f.write(word+\" \")\n",
    "\n",
    "\n",
    "f.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b614c44-aa79-4d22-bf98-b7c15f057bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0921fd8-b4f9-421e-92ae-da13267e5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18d1bd-c494-4793-92df-7acd89c3cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a2d5c-59bd-4e3b-ac13-d2c1406a0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "file1 = open(\"Answer.txt\") \n",
    " \n",
    "# Use this to read file content as a stream: \n",
    "line = file1.read()\n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('filteredtext.txt','a+') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504b85c-019a-4a6a-a945-df67ca78c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math \n",
    "# import string \n",
    "# import sys \n",
    "\n",
    "# # reading the text file \n",
    "# # This functio will return a \n",
    "# # list of the lines of text \n",
    "# # in the file. \n",
    "# def read_file(filename): \n",
    "\t\n",
    "# \ttry: \n",
    "# \t\twith open(filename, 'r') as f: \n",
    "# \t\t\tdata = f.read() \n",
    "# \t\treturn data \n",
    "\t\n",
    "# \texcept IOError: \n",
    "# \t\tprint(\"Error opening or reading input file: \", filename) \n",
    "# \t\tsys.exit() \n",
    "\n",
    "# # splitting the text lines into words \n",
    "# # translation table is a global variable \n",
    "# # mapping upper case to lower case and \n",
    "# # punctuation to spaces \n",
    "# translation_table = str.maketrans(string.punctuation+string.ascii_uppercase, \n",
    "# \t\t\t\t\t\t\t\t\t\" \"*len(string.punctuation)+string.ascii_lowercase) \n",
    "\t\n",
    "# # returns a list of the words \n",
    "# # in the file \n",
    "# def get_words_from_line_list(text): \n",
    "\t\n",
    "# \ttext = text.translate(translation_table) \n",
    "# \tword_list = text.split() \n",
    "\t\n",
    "# \treturn word_list \n",
    "\n",
    "\n",
    "# # counts frequency of each word \n",
    "# # returns a dictionary which maps \n",
    "# # the words to their frequency. \n",
    "# def count_frequency(word_list): \n",
    "\t\n",
    "# \tD = {} \n",
    "\t\n",
    "# \tfor new_word in word_list: \n",
    "\t\t\n",
    "# \t\tif new_word in D: \n",
    "# \t\t\tD[new_word] = D[new_word] + 1\n",
    "\t\t\t\n",
    "# \t\telse: \n",
    "# \t\t\tD[new_word] = 1\n",
    "\t\t\t\n",
    "# \treturn D \n",
    "\n",
    "# # returns dictionary of (word, frequency) \n",
    "# # pairs from the previous dictionary. \n",
    "# def word_frequencies_for_file(filename): \n",
    "\t\n",
    "# \tline_list = read_file(filename) \n",
    "# \tword_list = get_words_from_line_list(line_list) \n",
    "# \tfreq_mapping = count_frequency(word_list) \n",
    "\n",
    "# \tprint(\"File\", filename, \":\", ) \n",
    "# \tprint(len(line_list), \"lines, \", ) \n",
    "# \tprint(len(word_list), \"words, \", ) \n",
    "# \tprint(len(freq_mapping), \"distinct words\") \n",
    "\n",
    "# \treturn freq_mapping \n",
    "\n",
    "\n",
    "# # returns the dot product of two documents \n",
    "# def dotProduct(D1, D2): \n",
    "# \tSum = 0.0\n",
    "\t\n",
    "# \tfor key in D1: \n",
    "\t\t\n",
    "# \t\tif key in D2: \n",
    "# \t\t\tSum += (D1[key] * D2[key]) \n",
    "\t\t\t\n",
    "# \treturn Sum\n",
    "\n",
    "# # returns the angle in radians \n",
    "# # between document vectors \n",
    "# def vector_angle(D1, D2): \n",
    "# \tnumerator = dotProduct(D1, D2) \n",
    "# \tdenominator = math.sqrt(dotProduct(D1, D1)*dotProduct(D2, D2)) \n",
    "\t\n",
    "# \treturn math.acos(numerator / denominator) \n",
    "\n",
    "\n",
    "# def documentSimilarity(filename_1, filename_2): \n",
    "\t\n",
    "# # filename_1 = sys.argv[1] \n",
    "# # filename_2 = sys.argv[2] \n",
    "# \tsorted_word_list_1 = word_frequencies_for_file(filename_1) \n",
    "# \tsorted_word_list_2 = word_frequencies_for_file(filename_2) \n",
    "# \tdistance = vector_angle(sorted_word_list_1, sorted_word_list_2) \n",
    "\t\n",
    "# \tprint(\"The distance between the documents is: % 0.6f (radians)\"% distance) \n",
    "\t\n",
    "# # Driver code \n",
    "# documentSimilarity('Filtered.txt', 'Keywords.txt') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b112f65-9974-406a-b275-63c61f7037ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da501095-648c-4261-9ea2-c795e692f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900a51e-def6-44d8-8079-eaa5b3ee2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_file(filename): \n",
    "\t\n",
    "# \ttry: \n",
    "# \t\twith open(filename, 'r') as f: \n",
    "# \t\t\tdata = f.read()\n",
    "# \t\treturn data \n",
    "\t\n",
    "# \texcept IOError: \n",
    "# \t\tprint(\"Error opening or reading input file: \", filename) \n",
    "# \t\tsys.exit() \n",
    "# # Define the calculate_similarity function\n",
    "# def calculate_similarity(sentence1, sentence2):\n",
    "#     # Tokenize the sentences\n",
    "#     tokens1 = simple_preprocess(sentence1)\n",
    "#     tokens2 = simple_preprocess(sentence2)\n",
    "\n",
    "#     # Load or train a Word2Vec model\n",
    "#     # Here, we'll create a simple model for demonstration purposes\n",
    "#     sentences = [tokens1, tokens2]\n",
    "#     model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "#     # Calculate the vector representation for each sentence\n",
    "#     vector1 = np.mean([model.wv[token] for token in tokens1], axis=0)\n",
    "#     vector2 = np.mean([model.wv[token] for token in tokens2], axis=0)\n",
    "\n",
    "#     # Calculate cosine similarity\n",
    "#     similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "#     return similarity\n",
    "\n",
    "# # Example usage\n",
    "# sentence1 = read_file(\"Filtered.txt\")\n",
    "# sentence2 = read_file(\"Keywords.txt\")\n",
    "# similarity_score = calculate_similarity(sentence1, sentence2)\n",
    "# print(\"Similarity score:\", similarity_score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8561677-5241-4279-8bda-b7d15e8e92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the file in read mode \n",
    "filtered = open(\"filtered.txt\", \"r\") \n",
    "keywords = open(\"Keywords.txt\", \"r\")\n",
    "  \n",
    "# reading the file \n",
    "data1 = filtered .read() \n",
    "data2 = keywords.read()\n",
    "  \n",
    "# replacing end splitting the text  \n",
    "# when newline ('\\n') is seen. \n",
    "filtered_list = data1.split(\" \") \n",
    "#print(filtered_list) \n",
    "filtered.close() \n",
    "\n",
    "keyword_list = data2.split(\" \") \n",
    "#print(keyword_list) \n",
    "keywords.close() \n",
    "\n",
    "# using set intersection to get number of identical elements\n",
    "res = len(set(filtered_list) & set(keyword_list))\n",
    " \n",
    "# printing result\n",
    "print(\"Summation of Identical elements : \" + str(res))\n",
    "\n",
    "num_words = 0\n",
    " \n",
    "with open(\"Keywords.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        words = line.split()\n",
    "        num_words += len(words)\n",
    "print(\"Number of words:\")\n",
    "print(num_words)\n",
    "print(\"Percentage of similarity:\", (res/num_words)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c30f6-bc65-4e6b-a120-a57e5dfed99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
